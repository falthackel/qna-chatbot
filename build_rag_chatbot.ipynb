{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "- Build basic RAG chain\n",
    "- Build RAG chain that also returns the sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Project\\QnA_Chatbot\\qna-chatbot\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import torch\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov  9 16:50:34 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.90                 Driver Version: 565.90         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   44C    P8              3W /   75W |       0MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cu118\n",
      "CUDA available: True\n",
      "__CUDNN VERSION: 8700\n",
      "__Number CUDA Devices: 1\n",
      "__CUDA Device Name: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "__CUDA Device Total Memory [GB]: 6.441926656\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"CUDA available:\",use_cuda)\n",
    "if use_cuda:\n",
    "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "    print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "    print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if CUDA is not activate\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create Basic RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate embeddings model\n",
    "embeddings_model = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model='text-embedding-ada-002', max_retries=100, chunk_size=16, show_progress_bar=False)\n",
    "\n",
    "# Instantiate chat model\n",
    "chat_model = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0, model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chroma from disk\n",
    "vectorstore = Chroma(persist_directory=\"chroma_data\", embedding_function=embeddings_model)\n",
    "\n",
    "# Set up the vectorstore to be the retriever\n",
    "k = 5\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":k})\n",
    "\n",
    "# Format docs function\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Test out the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the retriever\n",
    "query = \"music streaming platform\"\n",
    "results = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good music streaming platform\n",
      "\n",
      "Great music streaming platform\n",
      "\n",
      "Best streaming music platform\n",
      "\n",
      "Best music streaming platform\n",
      "\n",
      "Best music streaming platform\n"
     ]
    }
   ],
   "source": [
    "# How would the formatted documents be rendered in the query?\n",
    "print(format_docs(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt from the langchain hub. Or you could write your own!\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_template_str = \"\"\"You are an advanced Q&A assistant specialized in analyzing user reviews for a music streaming application\n",
    "similar to Spotify. Your role is to extract meaningful and actionable insights from a large dataset of 3.4 million Google Store reviews.\n",
    "These insights should help management understand what users like, dislike, compare, and suggest about the application.\n",
    "\n",
    "Your responses should be accurate, clear, and concise, directly addressing the management's inquiries. Ensure your answers cover the\n",
    "following aspects:\n",
    "1. Positive aspects and features users appreciate most.\n",
    "2. Comparisons made with competitor apps (e.g., Pandora).\n",
    "3. Common complaints and areas of dissatisfaction among users.\n",
    "4. Emerging trends or patterns that could influence product strategy.\n",
    "\n",
    "In addition to your answers, provide a brief confidence score (out of 10) for each response to indicate the answerâ€™s reliability.\n",
    "\n",
    "Example responses to management questions might look like:\n",
    "- **Q**: \"What are the features users appreciate most?\"\n",
    "  **A**: \"Users often praise the intuitive UI design, extensive variety of music options, and seamless listening experience.\"\n",
    "\n",
    "- **Q**: \"Which platforms do users compare us with most?\"\n",
    "  **A**: \"Users frequently compare the application with Pandora, especially regarding feature availability and user experience.\"\n",
    "\n",
    "{context}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=review_template_str,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_human_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"{question}\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [review_system_prompt, review_human_prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_prompt_template = ChatPromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    messages=messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Test out the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | review_prompt_template\n",
    "    | chat_model\n",
    "    | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Q**: \"What are the features users appreciate most?\"  \\n**A**: \"Users appreciate the vast music library, personalized playlists, and high-quality audio streaming. Additionally, the offline listening feature and user-friendly interface are frequently highlighted as major positives.\"  \\n**Confidence Score**: 9/10  \\n\\n**Q**: \"Which platforms do users compare us with most?\"  \\n**A**: \"The application is often compared to Apple Music and Pandora, with users noting differences in music discovery features and subscription pricing.\"  \\n**Confidence Score**: 8/10  \\n\\n**Q**: \"What are the common complaints and areas of dissatisfaction among users?\"  \\n**A**: \"Common complaints include issues with song availability, occasional app crashes, and the presence of ads in the free version. Users also express dissatisfaction with the recommendation algorithm not aligning with their music preferences.\"  \\n**Confidence Score**: 9/10  \\n\\n**Q**: \"What emerging trends or patterns could influence product strategy?\"  \\n**A**: \"There is a growing trend towards integrating social features, such as sharing playlists and collaborative listening. Users are also increasingly interested in exclusive content and live streaming events, which could enhance user engagement.\"  \\n**Confidence Score**: 8/10  '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"music streaming platform\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Create Simple Gradio Answer Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate answer\n",
    "def generate_answer(message, history):\n",
    "    return rag_chain.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Project\\QnA_Chatbot\\qna-chatbot\\myenv\\Lib\\site-packages\\gradio\\components\\chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up chat bot interface\n",
    "answer_bot = gr.ChatInterface(\n",
    "                            generate_answer,\n",
    "                            chatbot=gr.Chatbot(height=300),\n",
    "                            textbox=gr.Textbox(placeholder=\"Ask me a questions\", container=False, scale=7),\n",
    "                            title=\"Q&A Chatbot\",\n",
    "                            description=\"Ask for meaningful information from the Google Store Reviews\",\n",
    "                            theme=\"soft\",\n",
    "                            examples=[\"music streaming platform\", \"trends\", \"features\"],\n",
    "                            cache_examples=False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://c73ea6228eeec1eadc.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c73ea6228eeec1eadc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_bot.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
